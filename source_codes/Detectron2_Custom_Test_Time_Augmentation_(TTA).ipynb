{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detectron2 Custom Test Time Augmentation (TTA)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKVOOxScRqMB"
      },
      "source": [
        "### <b>Detectron2 설치</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXiTnQMFRiBp"
      },
      "source": [
        "!pip install pyyaml==5.1\n",
        "\n",
        "# PyTorch 1.9.0 버전에 맞는 Detectron2 설치하기\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n",
        "\n",
        "# 설치가 완료되면 런타임 재시작하기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fptf7b5zRsQN"
      },
      "source": [
        "# PyTorch 설치 정보 확인 (1.9.0 버전이 본 실습의 기본 설정)\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju4Uy36gR4hs"
      },
      "source": [
        "# Detectron2 로깅(logging) 설정\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# 일반적으로 많이 사용되는 라이브러리 불러오기\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# 일반적으로 많이 사용되는 Detectron2 라이브러리 불러오기\n",
        "from detectron2 import model_zoo # Detectron2 모델\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyTwrwubSINr"
      },
      "source": [
        "### <b>사전 학습된 Detectron2 모델 불러오기</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXtlpguKSOzb"
      },
      "source": [
        "* Detectron2 환경설정(config) 정보를 생성한 뒤에 DefaultPredictor를 이용해 이미지에 대하여 추론합니다.\n",
        "* [Detectron2 기본 환경설정 정보 확인하기](https://detectron2.readthedocs.io/en/latest/modules/config.html#config-references)\n",
        "    * 참고로 cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST의 기본 값은 0.05입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr2yCoJySMds",
        "outputId": "1588f6aa-0a9f-407c-f064-f084b38dd406"
      },
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "# Detectron2 모델을 찾아 학습된 가중치 불러오기\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_final_280758.pkl: 167MB [00:04, 35.1MB/s]                           \n",
            "The checkpoint state_dict contains keys that are not used by the model:\n",
            "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i-VuUahSQzc"
      },
      "source": [
        "outputs = predictor(im) # 이미지를 모델에 넣어 결과 계산하기\n",
        "\n",
        "# 결과 출력하기\n",
        "print(outputs[\"instances\"].pred_classes)\n",
        "print(outputs[\"instances\"].pred_boxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV95iPh_SRpU"
      },
      "source": [
        "# Visualizer를 이용해 이미지와 함께 예측 결과를 출력하기\n",
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsb94V6VSaQh"
      },
      "source": [
        "### <b>데이터셋 등록하기</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfgf4_7USbCz"
      },
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "register_coco_instances(\"my_coco_val2017_1000\", {}, \"my_coco_val2017_1000/annotations/instances.json\", \"my_coco_val2017_1000/data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZzT4KvJSZiZ"
      },
      "source": [
        "### <b>평가 진행하기</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHEW2dXnSSrU"
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "evaluator = COCOEvaluator(\"my_coco_val2017_1000\") # output_dir 인자 값이 없으면 오류 발생할 수 있음\n",
        "val_loader = build_detection_test_loader(cfg, \"my_coco_val2017_1000\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clp7kxBPSwU6"
      },
      "source": [
        "#### <b>Test-Time Augmentation (TTA)</b>\n",
        "\n",
        "* Detectron2에서의 [TTA 구현](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/test_time_augmentation.py)을 참고할 수 있습니다.\n",
        "* Detectron2의 [modeling 라이브러리](https://detectron2.readthedocs.io/en/latest/modules/modeling.html#detectron2.modeling.GeneralizedRCNNWithTTA)를 참고할 수 있습니다.\n",
        "* Detectron2의 [TTA 관련 질문](https://github.com/facebookresearch/detectron2/issues?q=TTA)을 참고할 수 있습니다.\n",
        "* [Detectron2 기본 환경설정 정보 확인하기](https://detectron2.readthedocs.io/en/latest/modules/config.html#config-references)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK8ASwGhgyEL"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "from contextlib import contextmanager\n",
        "from itertools import count\n",
        "from typing import List\n",
        "import torch\n",
        "from fvcore.transforms import HFlipTransform, NoOpTransform\n",
        "from torch import nn\n",
        "from torch.nn.parallel import DistributedDataParallel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej70xxuQb_EK"
      },
      "source": [
        "from detectron2.config import configurable\n",
        "from detectron2.data.detection_utils import read_image\n",
        "from detectron2.data.transforms import (\n",
        "    RandomFlip,\n",
        "    ResizeShortestEdge,\n",
        "    ResizeTransform,\n",
        "    apply_augmentations,\n",
        ")\n",
        "from detectron2.structures import Boxes, Instances\n",
        "from detectron2.modeling.meta_arch import GeneralizedRCNN\n",
        "from detectron2.modeling.postprocessing import detector_postprocess\n",
        "from detectron2.modeling.roi_heads.fast_rcnn import fast_rcnn_inference_single_image\n",
        "\n",
        "\n",
        "# Test-Time Augmentation을 구현한 클래스\n",
        "class MyDatasetMapperTTA:\n",
        "    \"\"\"\n",
        "    이 클래스는 Detection 데이터셋을 입력으로 받고,\n",
        "    config에 명시된 augmentation 기법이 적용된 이미지 리스트를 출력으로 내보냅니다.\n",
        "    \"\"\"\n",
        "\n",
        "    @configurable\n",
        "    def __init__(self, min_sizes: List[int], max_size: int, flip: bool):\n",
        "        \"\"\"\n",
        "        [ResizeShortestEdge]: multi-scale 방법으로, 짧은 모서리를 기준으로 resize를 수행\n",
        "            - min_sizes: list of short-edge size to resize the image to\n",
        "            - max_size: maximum height or width of resized images\n",
        "        [RandomFlip]: 좌우 반전(horizontal flip)이 기본 설정\n",
        "            - flip: whether to apply flipping augmentation\n",
        "        \"\"\"\n",
        "        self.min_sizes = min_sizes\n",
        "        self.max_size = max_size\n",
        "        self.flip = flip\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg):\n",
        "        return {\n",
        "            \"min_sizes\": cfg.TEST.AUG.MIN_SIZES,\n",
        "            \"max_size\": cfg.TEST.AUG.MAX_SIZE,\n",
        "            \"flip\": cfg.TEST.AUG.FLIP,\n",
        "        }\n",
        "\n",
        "    def __call__(self, dataset_dict):\n",
        "        \"\"\"\n",
        "       입력 이미지를 받고, augmentation 기법이 적용된 이미지를 내보냅니다.\n",
        "            - 출력 dict 개수(기본 설정) = len(min_sizes) * (2 if flip else 1) = 18개\n",
        "            - 각 dict는 \"transforms\" field를 포함하고, 해당 이미지를 만들 때 어떤 transforms (tfms)이 사용되었는지 기록\n",
        "        \"\"\"\n",
        "        # transformation을 적용하기 위해 numpy 객체로 변환\n",
        "        numpy_image = dataset_dict[\"image\"].permute(1, 2, 0).numpy()\n",
        "        shape = numpy_image.shape\n",
        "        orig_shape = (dataset_dict[\"height\"], dataset_dict[\"width\"])\n",
        "        if shape[:2] != orig_shape:\n",
        "            # 원본 이미지를 input image로 변형하는 기본적인 함수(ResizeTransform)\n",
        "            pre_tfm = ResizeTransform(orig_shape[0], orig_shape[1], shape[0], shape[1])\n",
        "        else:\n",
        "            pre_tfm = NoOpTransform()\n",
        "\n",
        "        # 적용한 모든 augmentation의 조합(combination) 생성하기\n",
        "        aug_candidates = [] # each element is a list[Augmentation] (augmentation들의 리스트 형태)\n",
        "        for min_size in self.min_sizes:\n",
        "            resize = ResizeShortestEdge(min_size, self.max_size)\n",
        "            aug_candidates.append([resize]) # resize only\n",
        "            if self.flip:\n",
        "                flip = RandomFlip(prob=1.0)\n",
        "                aug_candidates.append([resize, flip]) # resize + flip\n",
        "        # (핵심) aug_candidates에 사용할 augmentations들을 리스트 형태로 삽입하면 구현 끝\n",
        "        print(f'Augmentation 개수: {len(aug_candidates)} ', aug_candidates)\n",
        "\n",
        "        # 모든 augmentation을 실제로 적용하기\n",
        "        ret = []\n",
        "        for aug in aug_candidates:\n",
        "            # 입력 값: (연달아 적용할 augmentation들, 이미지 numpy 객체)\n",
        "            # 반환 값: (augmentation을 적용한 이미지, 사용된 transforms 방법)\n",
        "            new_image, tfms = apply_augmentations(aug, np.copy(numpy_image))\n",
        "            torch_image = torch.from_numpy(np.ascontiguousarray(new_image.transpose(2, 0, 1))) # 다시 PyTorch 객체 형태로 변환\n",
        "\n",
        "            dic = copy.deepcopy(dataset_dict)\n",
        "            dic[\"transforms\"] = pre_tfm + tfms # 사용한 transforms 내용\n",
        "            dic[\"image\"] = torch_image # 결과적으로 만들어진 학습 이미지\n",
        "            ret.append(dic)\n",
        "        return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTXNobH3g5BY"
      },
      "source": [
        "class GeneralizedRCNNWithTTA(nn.Module):\n",
        "    \"\"\"\n",
        "    GeneralizedRCNN에 TTA를 적용하도록 해주는 클래스입니다. GeneralizedRCNN은 다음의 3단계로 구성됩니다.\n",
        "        - GeneralizedRCNN: Per-image feature extraction (backbone) → Region proposal generation → Per-region feature extraction and prediction\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg, model, tta_mapper=None, batch_size=3):\n",
        "        \"\"\"\n",
        "        (cfg, GeneralizedRCNN, tta_mapper, batch_size)를 입력으로 받습니다.\n",
        "            - tta_mapper는 기본적으로 DatasetMapperTTA(cfg)를 사용합니다.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if isinstance(model, DistributedDataParallel):\n",
        "            model = model.module\n",
        "        assert isinstance(\n",
        "            model, GeneralizedRCNN\n",
        "        ), \"TTA is only supported on GeneralizedRCNN. Got a model of type {}\".format(type(model))\n",
        "        self.cfg = cfg.clone()\n",
        "        # 현재는 기본적인 RCNN 기반의 Object Detection에 최적화되어 있음\n",
        "        assert not self.cfg.MODEL.KEYPOINT_ON, \"TTA for keypoint is not supported yet\"\n",
        "        assert (\n",
        "            not self.cfg.MODEL.LOAD_PROPOSALS\n",
        "        ), \"TTA for pre-computed proposals is not supported yet\"\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        if tta_mapper is None:\n",
        "            tta_mapper = DatasetMapperTTA(cfg) # 기본적으로 DatasetMapperTTA(cfg)를 사용(resize + flip)\n",
        "        self.tta_mapper = tta_mapper\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    @contextmanager\n",
        "    def _turn_off_roi_heads(self, attrs):\n",
        "        \"\"\"\n",
        "        model.roi_heads에서 일시적으로 몇몇 head를 끄고, 그 상태의 context를 엽니다.\n",
        "            - 예를 들어 \"mask_on\", \"keypoint_on\"과 같은 head를 끌 수 있습니다.\n",
        "        \"\"\"\n",
        "        roi_heads = self.model.roi_heads\n",
        "        old = {}\n",
        "        for attr in attrs:\n",
        "            try:\n",
        "                old[attr] = getattr(roi_heads, attr)\n",
        "            except AttributeError:\n",
        "                # The head may not be implemented in certain ROIHeads\n",
        "                pass\n",
        "\n",
        "        if len(old.keys()) == 0:\n",
        "            yield\n",
        "        else:\n",
        "            for attr in old.keys():\n",
        "                setattr(roi_heads, attr, False)\n",
        "            yield\n",
        "            for attr in old.keys():\n",
        "                setattr(roi_heads, attr, old[attr])\n",
        "\n",
        "    def _batch_inference(self, batched_inputs, detected_instances=None):\n",
        "        \"\"\"\n",
        "        입력 리스트에 대하여 추론(inference)을 수행합니다.\n",
        "        GeneralizedRCNN.inference()와 동일한 입력/출력 형식을 갖습니다.\n",
        "        \"\"\"\n",
        "        if detected_instances is None:\n",
        "            detected_instances = [None] * len(batched_inputs)\n",
        "\n",
        "        outputs = []\n",
        "        inputs, instances = [], []\n",
        "        for idx, input, instance in zip(count(), batched_inputs, detected_instances):\n",
        "            inputs.append(input)\n",
        "            instances.append(instance)\n",
        "            if len(inputs) == self.batch_size or idx == len(batched_inputs) - 1:\n",
        "                outputs.extend(\n",
        "                    self.model.inference(\n",
        "                        inputs,\n",
        "                        instances if instances[0] is not None else None,\n",
        "                        do_postprocess=False,\n",
        "                    )\n",
        "                )\n",
        "                inputs, instances = [], []\n",
        "        return outputs\n",
        "\n",
        "    def __call__(self, batched_inputs):\n",
        "        \"\"\"\n",
        "        GeneralizedRCNN.forward() 메서드와 동일한 input/output 형식을 가집니다.\n",
        "        \"\"\"\n",
        "        def _maybe_read_image(dataset_dict):\n",
        "            ret = copy.copy(dataset_dict)\n",
        "            if \"image\" not in ret:\n",
        "                image = read_image(ret.pop(\"file_name\"), self.model.input_format)\n",
        "                image = torch.from_numpy(np.ascontiguousarray(image.transpose(2, 0, 1)))  # CHW\n",
        "                ret[\"image\"] = image\n",
        "            if \"height\" not in ret and \"width\" not in ret:\n",
        "                ret[\"height\"] = image.shape[1]\n",
        "                ret[\"width\"] = image.shape[2]\n",
        "            return ret\n",
        "\n",
        "        # 각 이미지에 대하여 _inference_one_image() 메서드를 적용한 결과 반환\n",
        "        return [self._inference_one_image(_maybe_read_image(x)) for x in batched_inputs]\n",
        "\n",
        "    def _inference_one_image(self, input):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input (dict): one dataset dict with \"image\" field being a CHW tensor\n",
        "        Returns:\n",
        "            dict: one output dict\n",
        "        \"\"\"\n",
        "        orig_shape = (input[\"height\"], input[\"width\"])\n",
        "        augmented_inputs, tfms = self._get_augmented_inputs(input) # augmented 이미지를 확인\n",
        "        # 모든 augmented 이미지에 대하여 bounding box를 예측\n",
        "        with self._turn_off_roi_heads([\"mask_on\", \"keypoint_on\"]):\n",
        "            # 일시적으로 roi head를 끈 상태로 예측 (bounding box를 얻음)\n",
        "            all_boxes, all_scores, all_classes = self._get_augmented_boxes(augmented_inputs, tfms)\n",
        "        # 바운딩 박스에 대한 최종 예측 결과를 얻기 위해, 모든 검출 박스를 합치기 (NMS 진행)\n",
        "        merged_instances = self._merge_detections(all_boxes, all_scores, all_classes, orig_shape)\n",
        "\n",
        "        if self.cfg.MODEL.MASK_ON:\n",
        "            # Use the detected boxes to obtain masks\n",
        "            augmented_instances = self._rescale_detected_boxes(\n",
        "                augmented_inputs, merged_instances, tfms\n",
        "            )\n",
        "            # run forward on the detected boxes\n",
        "            outputs = self._batch_inference(augmented_inputs, augmented_instances)\n",
        "            # Delete now useless variables to avoid being out of memory\n",
        "            del augmented_inputs, augmented_instances\n",
        "            # average the predictions\n",
        "            merged_instances.pred_masks = self._reduce_pred_masks(outputs, tfms)\n",
        "            merged_instances = detector_postprocess(merged_instances, *orig_shape)\n",
        "            return {\"instances\": merged_instances}\n",
        "        else:\n",
        "            return {\"instances\": merged_instances}\n",
        "\n",
        "    def _get_augmented_inputs(self, input):\n",
        "        augmented_inputs = self.tta_mapper(input)\n",
        "        tfms = [x.pop(\"transforms\") for x in augmented_inputs]\n",
        "        return augmented_inputs, tfms\n",
        "\n",
        "    def _get_augmented_boxes(self, augmented_inputs, tfms):\n",
        "        # 1. 모든 augmented 이미지를 forward하여 결과를 얻기\n",
        "        outputs = self._batch_inference(augmented_inputs)\n",
        "        # 2. 얻은 결과 합치기(union)\n",
        "        all_boxes = []\n",
        "        all_scores = []\n",
        "        all_classes = []\n",
        "        for output, tfm in zip(outputs, tfms):\n",
        "            # (핵심) 결과를 합칠 때는, box에 적용된 transforms을 inverse해야 original image에 대한 결과를 얻을 수 있음\n",
        "            pred_boxes = output.pred_boxes.tensor\n",
        "            original_pred_boxes = tfm.inverse().apply_box(pred_boxes.cpu().numpy())\n",
        "            all_boxes.append(torch.from_numpy(original_pred_boxes).to(pred_boxes.device))\n",
        "\n",
        "            all_scores.extend(output.scores)\n",
        "            all_classes.extend(output.pred_classes)\n",
        "        all_boxes = torch.cat(all_boxes, dim=0)\n",
        "        return all_boxes, all_scores, all_classes\n",
        "\n",
        "    def _merge_detections(self, all_boxes, all_scores, all_classes, shape_hw):\n",
        "        # 모든 결과의 합(union)에서 선택\n",
        "        num_boxes = len(all_boxes)\n",
        "        num_classes = self.cfg.MODEL.ROI_HEADS.NUM_CLASSES\n",
        "        # fast_rcnn_inference()은 background scores를 이용하므로, 1만큼 더해주기\n",
        "        all_scores_2d = torch.zeros(num_boxes, num_classes + 1, device=all_boxes.device)\n",
        "        for idx, cls, score in zip(count(), all_classes, all_scores):\n",
        "            all_scores_2d[idx, cls] = score\n",
        "\n",
        "        # fast_rcnn_inference_single_image()는 non-maximum suppression (NMS)을 진행하는 메서드임\n",
        "        merged_instances, _ = fast_rcnn_inference_single_image(\n",
        "            all_boxes,\n",
        "            all_scores_2d,\n",
        "            shape_hw,\n",
        "            1e-8,\n",
        "            self.cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST,\n",
        "            self.cfg.TEST.DETECTIONS_PER_IMAGE,\n",
        "        )\n",
        "\n",
        "        return merged_instances\n",
        "\n",
        "    def _rescale_detected_boxes(self, augmented_inputs, merged_instances, tfms):\n",
        "        augmented_instances = []\n",
        "        for input, tfm in zip(augmented_inputs, tfms):\n",
        "            # Transform the target box to the augmented image's coordinate space\n",
        "            pred_boxes = merged_instances.pred_boxes.tensor.cpu().numpy()\n",
        "            pred_boxes = torch.from_numpy(tfm.apply_box(pred_boxes))\n",
        "\n",
        "            aug_instances = Instances(\n",
        "                image_size=input[\"image\"].shape[1:3],\n",
        "                pred_boxes=Boxes(pred_boxes),\n",
        "                pred_classes=merged_instances.pred_classes,\n",
        "                scores=merged_instances.scores,\n",
        "            )\n",
        "            augmented_instances.append(aug_instances)\n",
        "        return augmented_instances\n",
        "\n",
        "    def _reduce_pred_masks(self, outputs, tfms):\n",
        "        # Should apply inverse transforms on masks.\n",
        "        # We assume only resize & flip are used. pred_masks is a scale-invariant\n",
        "        # representation, so we handle flip specially\n",
        "        for output, tfm in zip(outputs, tfms):\n",
        "            if any(isinstance(t, HFlipTransform) for t in tfm.transforms):\n",
        "                output.pred_masks = output.pred_masks.flip(dims=[3])\n",
        "        all_pred_masks = torch.stack([o.pred_masks for o in outputs], dim=0)\n",
        "        avg_pred_masks = torch.mean(all_pred_masks, dim=0)\n",
        "        return avg_pred_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae3ps1YcC-NQ"
      },
      "source": [
        "* Basic Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heTkym6uSlek"
      },
      "source": [
        "my_dataset_mappter_tta = MyDatasetMapperTTA(\n",
        "    min_sizes=cfg.TEST.AUG.MIN_SIZES,\n",
        "    max_size=cfg.TEST.AUG.MAX_SIZE,\n",
        "    flip=cfg.TEST.AUG.FLIP,\n",
        ")\n",
        "tta_model = GeneralizedRCNNWithTTA(cfg, predictor.model, tta_mapper=my_dataset_mappter_tta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGW9pPcnSwqs"
      },
      "source": [
        "print(inference_on_dataset(tta_model, val_loader, evaluator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhsF0OYnIFDI"
      },
      "source": [
        "* 다음의 기능들을 사용할 수 있는 형태로 재구현\n",
        "    * RandomBrightness\n",
        "    * RandomContrast\n",
        "    * RandomFlip\n",
        "    * RandomSaturation\n",
        "    * RandomRotation\n",
        "    * ResizeShortestEdge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMEucgOjIKrY"
      },
      "source": [
        "from detectron2.config import configurable\n",
        "from detectron2.data.detection_utils import read_image\n",
        "from detectron2.data.transforms import (\n",
        "    RandomBrightness,\n",
        "    RandomContrast,\n",
        "    RandomFlip,\n",
        "    RandomSaturation,\n",
        "    RandomRotation,\n",
        "    ResizeShortestEdge,\n",
        "    ResizeTransform,\n",
        "    apply_augmentations,\n",
        ")\n",
        "from detectron2.structures import Boxes, Instances\n",
        "from detectron2.modeling.meta_arch import GeneralizedRCNN\n",
        "from detectron2.modeling.postprocessing import detector_postprocess\n",
        "from detectron2.modeling.roi_heads.fast_rcnn import fast_rcnn_inference_single_image\n",
        "\n",
        "\n",
        "# Test-Time Augmentation을 구현한 클래스\n",
        "class MyDatasetMapperTTA:\n",
        "    \"\"\"\n",
        "    이 클래스는 Detection 데이터셋을 입력으로 받고,\n",
        "    config에 명시된 augmentation 기법이 적용된 이미지 리스트를 출력으로 내보냅니다.\n",
        "    \"\"\"\n",
        "    @configurable\n",
        "    def __init__(self, selected: List[int], min_sizes: List[int], max_size: int, flip: bool):\n",
        "        \"\"\"\n",
        "        [RandomBrightness]: 랜덤으로 명도(brightness) 변경\n",
        "            - intensity_min: minimum augmentation\n",
        "            - intensity_max: maximum augmentation\n",
        "        [RandomContrast]: 랜덤으로 대조(contrast) 변경\n",
        "            - intensity_min: minimum augmentation\n",
        "            - intensity_max: maximum augmentation\n",
        "        [RandomFlip]: 좌우 반전(horizontal flip)이 기본 설정\n",
        "            - flip: whether to apply flipping augmentation\n",
        "        [RandomSaturation]: 랜덤으로 채도(saturation) 변경\n",
        "            - intensity_min: minimum augmentation\n",
        "            - intensity_max: maximum augmentation\n",
        "        [RandomRotation]: 랜덤으로 시계 방향 회전(rotation) 수행\n",
        "            - angle: [min, max] interval from which to sample the angle (in degrees)\n",
        "        [ResizeShortestEdge]: multi-scale 방법으로, 짧은 모서리를 기준으로 resize를 수행\n",
        "            - min_sizes: list of short-edge size to resize the image to\n",
        "            - max_size: maximum height or width of resized images\n",
        "        \"\"\"\n",
        "        self.selected = selected\n",
        "        self.min_sizes = min_sizes\n",
        "        self.max_size = max_size\n",
        "        self.flip = flip\n",
        "\n",
        "        print('[Your selection]')\n",
        "        for i in range(6):\n",
        "            if i == 0 and self.selected[i] == True:\n",
        "                print('RandomBrightness')\n",
        "            if i == 1 and self.selected[i] == True:\n",
        "                print('RandomContrast')\n",
        "            if i == 2 and self.selected[i] == True:\n",
        "                print('RandomFlip')\n",
        "            if i == 3 and self.selected[i] == True:\n",
        "                print('RandomSaturation')\n",
        "            if i == 4 and self.selected[i] == True:\n",
        "                print('RandomRotation')\n",
        "            if i == 5 and self.selected[i] == True:\n",
        "                print('ResizeShortestEdge')\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, cfg):\n",
        "        return {\n",
        "            \"min_sizes\": cfg.TEST.AUG.MIN_SIZES,\n",
        "            \"max_size\": cfg.TEST.AUG.MAX_SIZE,\n",
        "            \"flip\": cfg.TEST.AUG.FLIP,\n",
        "        }\n",
        "\n",
        "    def __call__(self, dataset_dict):\n",
        "        \"\"\"\n",
        "       입력 이미지를 받고, augmentation 기법이 적용된 이미지를 내보냅니다.\n",
        "            - 출력 dict 개수(기본 설정) = len(min_sizes) * (2 if flip else 1) = 18개\n",
        "            - 각 dict는 \"transforms\" field를 포함하고, 해당 이미지를 만들 때 어떤 transforms (tfms)이 사용되었는지 기록\n",
        "        \"\"\"\n",
        "        # transformation을 적용하기 위해 numpy 객체로 변환\n",
        "        numpy_image = dataset_dict[\"image\"].permute(1, 2, 0).numpy()\n",
        "        shape = numpy_image.shape\n",
        "        orig_shape = (dataset_dict[\"height\"], dataset_dict[\"width\"])\n",
        "        if shape[:2] != orig_shape:\n",
        "            # 원본 이미지를 input image로 변형하는 기본적인 함수(ResizeTransform)\n",
        "            pre_tfm = ResizeTransform(orig_shape[0], orig_shape[1], shape[0], shape[1])\n",
        "        else:\n",
        "            pre_tfm = NoOpTransform()\n",
        "\n",
        "        aug_candidates = [[]] # each element is a list[Augmentation] (augmentation들의 리스트 형태)\n",
        "        # 적용한 모든 augmentation의 조합(combination) 생성하기\n",
        "        for i in range(6):\n",
        "            if i == 0 and self.selected[i] == True:\n",
        "                temp = []\n",
        "                for _ in range(3):\n",
        "                    brightness = RandomBrightness(intensity_min=0.8, intensity_max=1.2)\n",
        "                    for aug in aug_candidates:\n",
        "                        temp.append(aug + [brightness])\n",
        "                aug_candidates = aug_candidates + temp\n",
        "            if i == 1 and self.selected[i] == True:\n",
        "                temp = []\n",
        "                for _ in range(3):\n",
        "                    contrast = RandomContrast(intensity_min=0.8, intensity_max=1.2)\n",
        "                    for aug in aug_candidates:\n",
        "                        temp.append(aug + [contrast])\n",
        "                aug_candidates = aug_candidates + temp\n",
        "            if i == 2 and self.selected[i] == True:\n",
        "                flip = RandomFlip(prob=1.0)\n",
        "                temp = []\n",
        "                for aug in aug_candidates:\n",
        "                    temp.append(aug + [flip])\n",
        "                aug_candidates = aug_candidates + temp\n",
        "            if i == 3 and self.selected[i] == True:\n",
        "                temp = []\n",
        "                for _ in range(3):\n",
        "                    saturation = RandomSaturation(intensity_min=0.8, intensity_max=1.2)\n",
        "                    for aug in aug_candidates:\n",
        "                        temp.append(aug + [saturation])\n",
        "                aug_candidates = aug_candidates + temp\n",
        "            if i == 4 and self.selected[i] == True:\n",
        "                temp = []\n",
        "                for _ in range(3):\n",
        "                    rotation = RandomRotation(angle=[-10, 10])\n",
        "                    for aug in aug_candidates:\n",
        "                        temp.append(aug + [rotation])\n",
        "                aug_candidates = aug_candidates + temp\n",
        "            if i == 5 and self.selected[i] == True:\n",
        "                temp = []\n",
        "                for min_size in self.min_sizes:\n",
        "                    resize = ResizeShortestEdge(min_size, self.max_size)\n",
        "                    for aug in aug_candidates:\n",
        "                        temp.append(aug + [resize])\n",
        "                aug_candidates = aug_candidates + temp\n",
        "        # (핵심) aug_candidates에 사용할 augmentations들을 리스트 형태로 삽입하면 구현 끝\n",
        "        print(f'Augmentation 개수: {len(aug_candidates)} ', aug_candidates)\n",
        "\n",
        "        # 모든 augmentation을 실제로 적용하기\n",
        "        ret = []\n",
        "        for aug in aug_candidates:\n",
        "            # 입력 값: (연달아 적용할 augmentation들, 이미지 numpy 객체)\n",
        "            # 반환 값: (augmentation을 적용한 이미지, 사용된 transforms 방법)\n",
        "            new_image, tfms = apply_augmentations(aug, np.copy(numpy_image))\n",
        "            torch_image = torch.from_numpy(np.ascontiguousarray(new_image.transpose(2, 0, 1))) # 다시 PyTorch 객체 형태로 변환\n",
        "\n",
        "            dic = copy.deepcopy(dataset_dict)\n",
        "            dic[\"transforms\"] = pre_tfm + tfms # 사용한 transforms 내용\n",
        "            dic[\"image\"] = torch_image # 결과적으로 만들어진 학습 이미지\n",
        "            ret.append(dic)\n",
        "        return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIFTy7hTTY7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f958bf1c-d40f-4469-eab1-51a1924e9736"
      },
      "source": [
        "my_dataset_mappter_tta = MyDatasetMapperTTA(\n",
        "    # RandomBrightness, RandomContrast, RandomFlip, RandomSaturation, RandomRotation, ResizeShortestEdge\n",
        "    [True, True, True, False, False, False],\n",
        "    min_sizes=cfg.TEST.AUG.MIN_SIZES,\n",
        "    max_size=cfg.TEST.AUG.MAX_SIZE,\n",
        "    flip=cfg.TEST.AUG.FLIP,\n",
        ")\n",
        "tta_model = GeneralizedRCNNWithTTA(cfg, predictor.model, tta_mapper=my_dataset_mappter_tta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Your selection]\n",
            "RandomBrightness\n",
            "RandomContrast\n",
            "RandomFlip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiaH5WooTcHy"
      },
      "source": [
        "print(inference_on_dataset(tta_model, val_loader, evaluator))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}